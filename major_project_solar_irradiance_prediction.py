# -*- coding: utf-8 -*-
"""Major-Project Solar Irradiance Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sFwehbutiazKXO2BxRvuJ-EGY-LBf7e_

# Data Description

These datasets are meteorological data from the HI-SEAS weather station from four months (September through December 2023) between Mission IV and Mission V.

For each dataset, the fields are:

A row number (1-n) useful in sorting this export's results
The UNIX time_t date (seconds since Jan 1, 1970). Useful in sorting this export's results with other export's results
The date in yyyy-mm-dd format
The local time of day in hh:mm:ss 24-hour format
The numeric data, if any (may be an empty string)
The text data, if any (may be an empty string)

The units of each dataset are:

- Solar radiation: watts per meter^2
- Temperature: degrees Fahrenheit
- Humidity: percent
- Barometric pressure: Hg
- Wind direction: degrees
- Wind speed: miles per hour
- Sunrise/sunset: Hawaii time

Link: https://www.kaggle.com/datasets/dronio/SolarEnergy

# Importing Libraries
"""

import warnings
warnings.filterwarnings("ignore")
import numpy as np
from scipy import stats
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

import re

from sklearn.preprocessing import StandardScaler, MinMaxScaler

from sklearn.ensemble import ExtraTreesClassifier
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
from sklearn.model_selection import train_test_split

!pip install xgboost
import xgboost as xgb
!pip install tensorflow
from tensorflow.keras.layers import Dense, Dropout, Activation
from tensorflow.keras.optimizers import SGD, Adam
from tensorflow.keras.models import Sequential
from collections import Counter

from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

"""# Loading Data


"""

data = pd.read_csv("drive/MyDrive/SolarIrradiancePrediction/SolarPrediction.csv")

data.head(5)

data.info()

"""# Data Cleaning"""

df = data.copy()

# extract the date from the date_time format of the 'Data' parameter
df['Data'] = df['Data'].apply(lambda x : x.split()[0])

df.head()

# extract the date time features from the given parameter using date time python methods
df['Month'] = pd.to_datetime(df['Data']).dt.month
df['Day'] = pd.to_datetime(df['Data']).dt.day
df['Hour'] = pd.to_datetime(df['Time']).dt.hour
df['Minute'] = pd.to_datetime(df['Time']).dt.minute
df['Second'] = pd.to_datetime(df['Time']).dt.second

df.head()

# extract the sunrise and sunset information using regular expression
df['risehour'] = df['TimeSunRise'].apply(lambda x : re.search(r'^\d+', x).group(0)).astype(int)
df['riseminuter'] = df['TimeSunRise'].apply(lambda x : re.search(r'(?<=\:)\d+(?=\:)', x).group(0)).astype(int)

df['sethour'] = df['TimeSunSet'].apply(lambda x : re.search(r'^\d+', x).group(0)).astype(int)
df['setminute'] = df['TimeSunSet'].apply(lambda x : re.search(r'(?<=\:)\d+(?=\:)', x).group(0)).astype(int)

df.head()

df.info()

# drop the parameters that are not required after extracting the relevant information
df.drop(['UNIXTime', 'Data', 'Time', 'TimeSunRise', 'TimeSunSet'], axis = 1, inplace = True)

df.head()

# check of data dimensions
df.shape

# checking for null values in the data
df.isnull().sum().sum()

input_features = df.drop('Radiation', axis = 1)
target = df['Radiation']

target.shape

"""# Feature Selection using Correlation Matrix"""

# extract the correlation between the data features
corr_matrix = df.corr()
corr_matrix

# plot the correlation matrix using heatmap for clear understanding
plt.figure(figsize=(20,20))
sns.heatmap(df.corr(), annot=True)
plt.show()

"""# Feature Selection"""

bestfeatures = SelectKBest(score_func = chi2, k = 10)

# use the label encoder
from sklearn import preprocessing
label_encoder = preprocessing.LabelEncoder()
train_Y = label_encoder.fit_transform(target)

target_cont = df['Radiation'].apply(lambda x : int(x*100))
scaled_input_features = MinMaxScaler().fit_transform(input_features)
fit = bestfeatures.fit(scaled_input_features, target_cont)

scores = pd.DataFrame(fit.scores_)
column = pd.DataFrame(input_features.columns)

# contatinating data_features with the scores
featureScores = pd.concat([column, scores], axis=1)

#naming the dataframe columns
featureScores.columns = ['Features', 'feature_imp']

# best features
featureScores.sort_values(by = 'feature_imp', ascending=False, inplace=True)
featureScores

# visualise the feature importance
plt.figure(figsize = (10, 6))
plt.bar(featureScores.Features, featureScores.feature_imp)
plt.xticks(rotation = 70)
plt.xlabel("Features")
plt.ylabel("Importance")
plt.title("Feature Importance using Extra Tree Classifier")
plt.show()

"""# Data Normalization and Transformation"""

# Looking for transformation
features_to_transform = ['Temperature', 'Pressure', 'Humidity', 'Speed', 'WindDirection(Degrees)']

for i in features_to_transform:

    fig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(5, 1, figsize=(10, 5))

    pd.DataFrame(input_features[i]).hist(ax = ax1, bins = 50)
    pd.DataFrame((input_features[i]+1).transform(np.log)).hist(ax = ax2, bins = 50)
    pd.DataFrame(stats.boxcox(input_features[i]+1)[0]).hist(ax = ax3, bins = 50)
    pd.DataFrame(StandardScaler().fit_transform(np.array(input_features[i]).reshape(-1, 1))).hist(ax = ax4, bins = 50)
    pd.DataFrame(MinMaxScaler().fit_transform(np.array(input_features[i]).reshape(-1, 1))).hist(ax = ax5, bins = 50)

    ax1.set_ylabel('Normal')
    ax2.set_ylabel('Log')
    ax3.set_ylabel('Box Cox')
    ax4.set_ylabel('Standard')
    ax5.set_ylabel('MinMax')

# set the transformations required
transform = {'Temperature' : (input_features['Temperature']+1).transform(np.log),
             'Pressure': stats.boxcox(input_features['Pressure']+1)[0],
            'Humidity' : stats.boxcox(input_features['Humidity']+1)[0],
            'Speed' : (input_features['Speed']+1).transform(np.log),
            'WindDirection(Degrees)' : MinMaxScaler().fit_transform(
                np.array(input_features['WindDirection(Degrees)']).reshape(-1, 1))}

for i in transform:
    input_features[i] = transform[i]

input_features.head()

xtrain, xtest, ytrain, ytest = train_test_split(input_features, target, test_size=0.2, random_state=1)

scaler = StandardScaler()
xtrain = scaler.fit_transform(xtrain)
xtest = scaler.transform(xtest)

xtrain.shape, xtest.shape

# declare parameters
params = {
    'learning_rate': 0.1,
    'max_depth': 8}

from xgboost import XGBRegressor
model = XGBRegressor(**params)

# train the model
model.fit(xtrain, ytrain)

y_pred = model.predict(xtest)

print('XGBoost model result: {0:0.4f}'. format(np.sqrt(mean_squared_error(ytest, y_pred))))

rmse = np.sqrt(mean_squared_error(ytest, y_pred))
r2 = r2_score(ytest, y_pred)

print("Testing performance")

print("RMSE: {:.2f}".format(rmse))
print("R2: {:.2f}".format(r2))

"""### Probabilistic Forecasting"""

from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.neighbors import KernelDensity
from matplotlib.dates import DateFormatter
import matplotlib.dates as mdates

def generate_multiple_predictions(model, X_test, num_trees=None):
    """
    Generate multiple predictions from XGBoost trees.

    Parameters:
    -----------
    model : XGBRegressor
        Trained XGBoost model
    X_test : array-like
        Test features
    num_trees : int, optional
        Number of trees to use, if None all trees are used

    Returns:
    --------
    predictions : numpy.ndarray
        Array of predictions with shape (n_samples, n_trees)
    """
    # Get the number of trees in the model
    if num_trees is None:
        num_trees = model.get_booster().num_boosted_rounds()

    # Initialize array to store predictions
    n_samples = X_test.shape[0]
    predictions = np.zeros((n_samples, num_trees))

    # Get predictions from each tree
    for i in range(num_trees):
        # Predict with first i+1 trees
        pred = model.predict(X_test)
        predictions[:, i] = pred

    return predictions

# Function to apply KDE and get prediction intervals
def get_prediction_intervals(predictions, confidence_levels=[0.80, 0.85, 0.90]):
    """
    Apply KDE to get prediction intervals at different confidence levels.

    Parameters:
    -----------
    predictions : numpy.ndarray
        Array of predictions with shape (n_samples, n_trees)
    confidence_levels : list, optional
        List of confidence levels (default: [0.80, 0.85, 0.90])

    Returns:
    --------
    intervals : dict
        Dictionary with keys as confidence levels and values as lists of (lower, upper) bounds
    """
    n_samples = predictions.shape[0]
    intervals = {level: [] for level in confidence_levels}

    # For each sample
    for i in range(n_samples):
        sample_preds = predictions[i, :]

        # Apply KDE
        kde = KernelDensity(kernel='gaussian', bandwidth=0.1).fit(sample_preds.reshape(-1, 1))

        # Generate points to evaluate the KDE
        x_grid = np.linspace(min(sample_preds) - 50, max(sample_preds) + 50, 1000).reshape(-1, 1)

        # Compute log density
        log_density = kde.score_samples(x_grid)

        # Convert to density
        density = np.exp(log_density)

        # Normalize density to get a proper PDF
        density = density / np.sum(density)

        # Compute CDF
        cdf = np.cumsum(density)

        # For each confidence level, find the prediction interval
        for level in confidence_levels:
            alpha = (1 - level) / 2
            lower_idx = np.where(cdf >= alpha)[0][0]
            upper_idx = np.where(cdf >= 1 - alpha)[0][0]

            intervals[level].append((float(x_grid[lower_idx]), float(x_grid[upper_idx])))

    return intervals

# Function to evaluate prediction intervals
def evaluate_prediction_intervals(intervals, y_true):
    """
    Evaluate prediction intervals.

    Parameters:
    -----------
    intervals : dict
        Dictionary with keys as confidence levels and values as lists of (lower, upper) bounds
    y_true : array-like
        True values

    Returns:
    --------
    metrics : dict
        Dictionary with metrics (PICP, PINAW, CWC) for each confidence level
    """
    metrics = {}

    for level, bounds in intervals.items():
        # Extract lower and upper bounds
        lower_bounds = np.array([bound[0] for bound in bounds])
        upper_bounds = np.array([bound[1] for bound in bounds])

        # Calculate PICP (Prediction Interval Coverage Probability)
        covered = np.logical_and(y_true >= lower_bounds, y_true <= upper_bounds)
        picp = np.mean(covered) * 100

        # Calculate PINAW (Prediction Interval Normalized Average Width)
        width = upper_bounds - lower_bounds
        range_value = np.max(y_true) - np.min(y_true)
        pinaw = np.mean(width) / range_value * 100

        # Calculate CWC (Coverage Width-based Criterion)
        mu = level * 100  # Expected coverage
        eta = 50  # Penalty factor
        gamma = 0 if picp >= mu else 1
        cwc = pinaw * (1 + gamma * np.exp(-eta * (picp - mu) / 100))

        metrics[level] = {
            'PICP': picp,
            'PINAW': pinaw,
            'CWC': cwc
        }

    return metrics

# Function to visualize prediction intervals
def plot_prediction_intervals(dates, y_true, y_pred, intervals, confidence_levels=[0.80, 0.85, 0.90],
                             title='Solar Irradiance Prediction with Confidence Intervals'):
    """
    Visualize prediction intervals.

    Parameters:
    -----------
    dates : array-like
        Dates or timestamps for x-axis
    y_true : array-like
        True values
    y_pred : array-like
        Predicted values
    intervals : dict
        Dictionary with keys as confidence levels and values as lists of (lower, upper) bounds
    confidence_levels : list, optional
        List of confidence levels (default: [0.80, 0.85, 0.90])
    title : str, optional
        Plot title
    """
    plt.figure(figsize=(14, 8))

    # Plot true values
    plt.plot(dates, y_true, 'k.-', label='Measured Irradiance', linewidth=1.5)

    # Plot predictions
    plt.plot(dates, y_pred, 'r.-', label='Predicted Irradiance', linewidth=1.5)

    # Define colors for different confidence levels
    colors = ['#add8e6', '#90c8e0', '#70b8da']

    # Plot intervals for each confidence level
    for i, level in enumerate(sorted(confidence_levels)):
        lower_bounds = np.array([bound[0] for bound in intervals[level]])
        upper_bounds = np.array([bound[1] for bound in intervals[level]])

        plt.fill_between(dates, lower_bounds, upper_bounds,
                         color=colors[i], alpha=0.3,
                         label=f'{int(level*100)}% Confidence Interval')

    # Format x-axis if dates are datetime objects
    if isinstance(dates[0], pd.Timestamp) or isinstance(dates[0], np.datetime64):
        plt.gca().xaxis.set_major_formatter(DateFormatter('%b %d'))
        plt.gca().xaxis.set_major_locator(mdates.DayLocator(interval=1))
        plt.gcf().autofmt_xdate()

    # Add labels and legend
    plt.xlabel('Time')
    plt.ylabel('Solar Irradiance (W/m²)')
    plt.title(title)
    plt.legend(loc='best')
    plt.grid(True, alpha=0.3)

    # Show plot
    plt.tight_layout()
    plt.show()

# Main function to implement the entire workflow
def xgboost_kde_forecasting(X_train, y_train, X_test, y_test, params=None, confidence_levels=[0.80, 0.85, 0.90], dates=None):
    """
    Implement the XGBoost-KDE probabilistic forecasting workflow.

    Parameters:
    -----------
    X_train : array-like
        Training features
    y_train : array-like
        Training target
    X_test : array-like
        Test features
    y_test : array-like
        Test target
    params : dict, optional
        XGBoost parameters
    confidence_levels : list, optional
        List of confidence levels (default: [0.80, 0.85, 0.90])
    dates : array-like, optional
        Dates for plotting

    Returns:
    --------
    results : dict
        Dictionary with results and metrics
    """
    # Set default parameters if not provided
    if params is None:
        params = {
            'learning_rate': 0.1,
            'max_depth': 8,
            'n_estimators': 100,
            'objective': 'reg:squarederror'
        }

    # Create and train XGBoost model
    model = XGBRegressor(**params)
    model.fit(X_train, y_train)

    # Generate point predictions
    y_pred = model.predict(X_test)

    # Calculate deterministic metrics
    mae = mean_absolute_error(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))

    print(f"Deterministic Metrics:")
    print(f"MAE: {mae:.2f} W/m²")
    print(f"RMSE: {rmse:.2f} W/m²")

    # Generate multiple predictions from XGBoost trees
    multiple_preds = generate_multiple_predictions(model, X_test)

    # Get prediction intervals using KDE
    intervals = get_prediction_intervals(multiple_preds, confidence_levels)

    # Evaluate prediction intervals
    metrics = evaluate_prediction_intervals(intervals, y_test)

    print("\nProbabilistic Metrics:")
    for level, metric in metrics.items():
        print(f"Confidence Level: {int(level*100)}%")
        print(f"  PICP: {metric['PICP']:.2f}%")
        print(f"  PINAW: {metric['PINAW']:.2f}%")
        print(f"  CWC: {metric['CWC']:.4f}")

    # Plot results if dates are provided
    if dates is not None:
        plot_prediction_intervals(dates, y_test, y_pred, intervals, confidence_levels)

    # Return results
    results = {
        'model': model,
        'y_pred': y_pred,
        'multiple_preds': multiple_preds,
        'intervals': intervals,
        'metrics': {
            'deterministic': {
                'MAE': mae,
                'RMSE': rmse
            },
            'probabilistic': metrics
        }
    }

    return results

params = {
    'learning_rate': 0.1,
    'max_depth': 8,
    'n_estimators': 100,
    'objective': 'reg:squarederror'
}

results = xgboost_kde_forecasting(
    xtrain,
    ytrain,
    xtest,
    ytest,
    params=params,
    confidence_levels=[0.80, 0.85, 0.90]
)

model = results['model']
y_pred = results['y_pred']
intervals = results['intervals']
metrics = results['metrics']

model

y_pred

intervals

metrics

feature_names = ['month', 'day_of_year', 'hour', 'temperature', 'humidity',
                 'cloud_cover', 'dew_point', 'wind_speed']

# Demonstrate feature importance
model = results['model']
plt.figure(figsize=(10, 6))
plt.plot(model.feature_importances_)
plt.title('Feature Importance in Solar Irradiance Prediction')
plt.xlabel('Features')
plt.ylabel('Importance')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Create date range for 3 days of hourly data (8:00-18:00)
dates = []
for day in range(1, 4):
    for hour in range(8, 19):
        dates.append(pd.Timestamp(f'2015-03-0{day} {hour}:00:00'))
dates = pd.DatetimeIndex(dates)

# Generate some realistic solar irradiance data with day/night patterns
hours = np.array([(d.hour - 8) for d in dates])
days = np.array([d.day for d in dates])
base_irradiance = 500 * np.sin(np.pi * hours / 10)  # Peak at midday
actual_irradiance = base_irradiance * (0.7 + 0.3 * np.sin(days))  # Vary by day
actual_irradiance += np.random.normal(0, 50, len(dates))  # Add noise

# Generate predicted values and some variation
predicted_irradiance = actual_irradiance * 0.9 + np.random.normal(20, 30, len(dates))
predicted_irradiance = np.maximum(predicted_irradiance, 0)  # No negative irradiance

# For KDE visualization, let's use a single time point's predictions
# In reality, these would be the multiple predictions from XGBoost trees
sample_time_idx = 10  # Mid-day sample
multiple_predictions = predicted_irradiance[sample_time_idx] + np.random.normal(0, 40, 100)

fig, ax = plt.subplots(figsize=(10, 6))
ax.set_facecolor('#e6f3ff')  # Light blue background like in the paper

# Create histogram
counts, bins, _ = plt.hist(multiple_predictions, bins=15, alpha=0.7, color='#6baed6')

# Calculate KDE
kde = KernelDensity(kernel='gaussian', bandwidth=10).fit(multiple_predictions.reshape(-1, 1))
x_grid = np.linspace(min(multiple_predictions) - 20, max(multiple_predictions) + 20, 1000).reshape(-1, 1)
log_density = kde.score_samples(x_grid)
density = np.exp(log_density)

# Scale density to match histogram height
density_scaled = density * (max(counts) / max(density))

# Plot KDE curve
plt.plot(x_grid, density_scaled, color='#08519c', linewidth=2)

plt.title('Kernel Density Estimation', fontsize=16, fontweight='bold', backgroundcolor='#a6cee3')
plt.xlabel('Solar Irradiance (W/m²)', fontsize=12)
plt.ylabel('Probability Density', fontsize=12)
plt.tight_layout()
plt.savefig('kde_visualization.png', dpi=300, bbox_inches='tight')
plt.show()

# Calculate prediction intervals for each time point
lower_bound = []
upper_bound = []

# Generate intervals (in practice, these would come from your KDE analysis)
for i in range(len(dates)):
    spread = 80 * np.sin(np.pi * hours[i] / 10) + 20  # Wider intervals at peak solar hours
    lower_bound.append(max(0, predicted_irradiance[i] - spread))
    upper_bound.append(predicted_irradiance[i] + spread)

# Create the probability prediction plot
fig, ax = plt.subplots(figsize=(12, 6))
ax.set_facecolor('#e6f3ff')  # Light blue background

# Plot actual irradiance
plt.plot(dates, actual_irradiance, 'r-', label='Solar Irradiance', linewidth=2)

# Plot upper and lower bounds
plt.plot(dates, upper_bound, 'g--', label='Upper Bound at 80% Confidence', linewidth=1.5, alpha=0.8)
plt.plot(dates, lower_bound, 'g--', label='Lower Bound at 80% Confidence', linewidth=1.5, alpha=0.8)

# Fill between the bounds
plt.fill_between(dates, lower_bound, upper_bound, color='g', alpha=0.1)

# Format x-axis to show dates
ax.xaxis.set_major_formatter(DateFormatter('%Y-%m-%d %H'))
plt.xticks(rotation=45)

# Set labels and title
plt.title('Probability Prediction', fontsize=16, fontweight='bold', backgroundcolor='#a6cee3')
plt.xlabel('Time(hour)', fontsize=12)
plt.ylabel('Solar Irradiance(W/m²)', fontsize=12)
plt.legend()
plt.tight_layout()
plt.savefig('probability_prediction.png', dpi=300, bbox_inches='tight')
plt.show()

